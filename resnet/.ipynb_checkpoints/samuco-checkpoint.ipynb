{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# note\n",
    "__[This](https://github.com/BVLC/caffe/blob/master/examples/02-fine-tuning.ipynb) is the original notebook where the following code was adopted from__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caffe_root = '/Users/samuco/Downloads/caffe-opencl/'\n",
    "data_root = '/Users/samuco/Downloads/caffe-opencl/cs229/data/'\n",
    "model_dir = '/Users/samuco/Desktop/CS229 - Final Project/state_farm/notebooks'\n",
    "\n",
    "# Defines\n",
    "VGG16 = 'vgg16'\n",
    "RESNET_50 = 'resnet50'\n",
    "RESNET_101 = 'resnet101'\n",
    "RESNET_152 = 'resnet152'\n",
    "ALEXNET = 'alexnet' # not setup yet\n",
    "GOOGLENET = 'googlenet'\n",
    "\n",
    "# Which model to train/test\n",
    "input_model = RESNET_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /Users/samuco/Desktop/CS229 - Final Project/state_farm/notebooks/ResNet-50-deploy.prototxt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "input_proto = ''\n",
    "if input_model == RESNET_50:\n",
    "    input_proto = os.path.join(model_dir, 'ResNet-50-deploy.prototxt')\n",
    "elif input_model == RESNET_101:\n",
    "    input_proto = 'ResNet-101-deploy.prototxt'\n",
    "elif input_model == RESNET_152:\n",
    "    input_proto = 'ResNet-152-deploy.prototxt'\n",
    "print \"Using\", input_proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuco/Downloads/caffe-opencl/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for std::__1::vector<int, std::__1::allocator<int> > already registered; second conversion method ignored.\n",
      "  from ._caffe import \\\n",
      "/Users/samuco/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    "\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "%matplotlib inline\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pretrained weights, labels, and mean image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var kernel = Jupyter.notebook.kernel; \n",
       "var command = [\"notebookPath = \",\n",
       "               \"'\", window.document.body.dataset.notebookPath, \"'\" ].join('')\n",
       "//alert(command)\n",
       "kernel.execute(command)\n",
       "var command = [\"notebookName = \",\n",
       "               \"'\", window.document.body.dataset.notebookName, \"'\" ].join('')\n",
       "//alert(command)\n",
       "kernel.execute(command)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript \n",
    "var kernel = Jupyter.notebook.kernel; \n",
    "var command = [\"notebookPath = \",\n",
    "               \"'\", window.document.body.dataset.notebookPath, \"'\" ].join('')\n",
    "//alert(command)\n",
    "kernel.execute(command)\n",
    "var command = [\"notebookName = \",\n",
    "               \"'\", window.document.body.dataset.notebookName, \"'\" ].join('')\n",
    "//alert(command)\n",
    "kernel.execute(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "--2016-12-10 20:35:58--  http://dl.caffe.berkeleyvision.org/caffe_ilsvrc12.tar.gz\n",
      "Resolving dl.caffe.berkeleyvision.org... 169.229.222.251\n",
      "Connecting to dl.caffe.berkeleyvision.org|169.229.222.251|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17858008 (17M) [application/octet-stream]\n",
      "Saving to: ‘caffe_ilsvrc12.tar.gz’\n",
      "\n",
      "caffe_ilsvrc12.tar. 100%[===================>]  17.03M  2.87MB/s    in 5.7s    \n",
      "\n",
      "2016-12-10 20:36:04 (3.01 MB/s) - ‘caffe_ilsvrc12.tar.gz’ saved [17858008/17858008]\n",
      "\n",
      "Unzipping...\n",
      "Done.\n",
      "/Users/samuco/Desktop/CS229 - Final Project/state_farm/notebooks\n",
      "You need to manually download the ResNet models from https://onedrive.live.com/?authkey=%21AAFW2-FVoxeVRck&id=4006CBB8476FF777%2117887&cid=4006CBB8476FF777\n"
     ]
    }
   ],
   "source": [
    "# This downloads the ilsvrc auxiliary data (mean file, etc),\n",
    "import os\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "os.chdir(caffe_root)  # run scripts from caffe root\n",
    "!data/ilsvrc12/get_ilsvrc_aux.sh\n",
    "#dirn = os.path.abspath(os.path.join(home, notebookPath, \"..\"))\n",
    "dirn = model_dir\n",
    "print(dirn)\n",
    "os.chdir(dirn)  # run scripts from caffe root\n",
    "\n",
    "if input_model == VGG16:\n",
    "    !scripts/download_model_binary.py ./\n",
    "elif input_model == RESNET_50 or input_model ==  RESNET_101 or input_model == RESNET_101:\n",
    "    print(\"You need to manually download the ResNet models from https://onedrive.live.com/?authkey=%21AAFW2-FVoxeVRck&id=4006CBB8476FF777%2117887&cid=4006CBB8476FF777\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/samuco/Desktop/CS229 - Final Project/state_farm/notebooks/ResNet-50-model.caffemodel\n"
     ]
    }
   ],
   "source": [
    "# Define weights, the path to the ImageNet pretrained weights we just downloaded, and make sure it exists.\n",
    "import os\n",
    "if input_model == VGG16:\n",
    "    weights = dirn + '/vgg16_reference_caffenet.caffemodel'\n",
    "elif input_model == RESNET_50:\n",
    "    weights = dirn + '/ResNet-50-model.caffemodel'\n",
    "else:\n",
    "    print \"Invalid model \" + input_model\n",
    "print(weights)\n",
    "assert os.path.exists(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load StateFarm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset is already exists\n"
     ]
    }
   ],
   "source": [
    "# download StateFarm data\n",
    "# Make sure you have read the README file under data directory\n",
    "import os\n",
    "os.chdir(data_root)\n",
    "if os.path.isdir('train'):\n",
    "    print \"dataset is already exists\"\n",
    "else:\n",
    "    !bash get_datasets.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV\n",
      "Splitting Data\n",
      "Writing CSV\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# define train and valid sets\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Reading CSV\")\n",
    "labels = pd.read_csv(data_root+'driver_imgs_list.csv')[['subject', 'img', 'classname']]\n",
    "labels['img'] = labels.apply(lambda row: data_root+'train/'+row.classname+'/'+row.img, 1)\n",
    "labels['classname'] = labels['classname'].map(lambda l: l[1])\n",
    "labels = labels.reindex(np.random.permutation(labels.index))\n",
    "\n",
    "# select a driver (say p002)\n",
    "print(\"Splitting Data\")\n",
    "test = labels[labels[\"subject\"] == \"p002\"][['img', 'classname']]\n",
    "train = labels[labels[\"subject\"] != \"p002\"][['img', 'classname']]\n",
    "\n",
    "# write the csv\n",
    "print(\"Writing CSV\")\n",
    "test.to_csv(data_root+'valid.txt', sep=' ', header=False, index=False)\n",
    "train.to_csv(data_root+'train.txt', sep=' ', header=False, index=False)\n",
    "labels = None\n",
    "\n",
    "NUM_STATEFARM_LABELS = 10\n",
    "kaggle_labels = [\n",
    "    'c0: safe driving',\n",
    "    'c1: texting - right',\n",
    "    'c2: talking on the phone - right',\n",
    "    'c3: texting - left',\n",
    "    'c4: talking on the phone - left',\n",
    "    'c5: operating the radio',\n",
    "    'c6: drinking',\n",
    "    'c7: reaching behind',\n",
    "    'c8: hair and makeup',\n",
    "    'c9: talking to passenger'\n",
    "]\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining caffe net\n",
    "caffenet, a function which initializes the CaffeNet architecture (a minor variant on AlexNet), taking arguments specifying the data and number of output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "weight_param = dict(lr_mult=1, decay_mult=1)\n",
    "bias_param   = dict(lr_mult=2, decay_mult=0)\n",
    "learned_param = [weight_param, bias_param]\n",
    "\n",
    "frozen_param = [dict(lr_mult=0)] * 2\n",
    "\n",
    "def conv_relu(bottom, ks, nout, stride=1, pad=0, group=1,\n",
    "              param=learned_param,\n",
    "              weight_filler=dict(type='gaussian', std=0.01),\n",
    "              bias_filler=dict(type='constant', value=0.1)):\n",
    "    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n",
    "                         num_output=nout, pad=pad, group=group,\n",
    "                         param=param, weight_filler=weight_filler,\n",
    "                         bias_filler=bias_filler)\n",
    "    return conv, L.ReLU(conv, in_place=True)\n",
    "\n",
    "def alex_conv_relu(bottom, ks, nout, stride=1, pad=0, group=1,\n",
    "              param=learned_param,\n",
    "              weight_filler=dict(type='gaussian', std=0.01),\n",
    "              bias_filler=dict(type='constant', value=0)):\n",
    "    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n",
    "                         num_output=nout, pad=pad, group=group,\n",
    "                         param=param, weight_filler=weight_filler,\n",
    "                         bias_filler=bias_filler)\n",
    "    return conv, L.ReLU(conv, in_place=True)\n",
    "\n",
    "def fc_relu(bottom, nout, param=learned_param,\n",
    "            weight_filler=dict(type='gaussian', std=0.005),\n",
    "            bias_filler=dict(type='constant', value=0.1)):\n",
    "    fc = L.InnerProduct(bottom, num_output=nout, param=param,\n",
    "                        weight_filler=weight_filler,\n",
    "                        bias_filler=bias_filler)\n",
    "    return fc, L.ReLU(fc, in_place=True)\n",
    "\n",
    "def max_pool(bottom, ks, stride=1):\n",
    "    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n",
    "    \n",
    "def vgg16_conv_relu(bottom, ks, nout, pad=0, group=1,\n",
    "              param=learned_param,\n",
    "              weight_filler=dict(type='xavier'),\n",
    "              bias_filler=dict(type='constant', value=0.0)):\n",
    "    conv = L.Convolution(bottom, kernel_size=ks,\n",
    "                         num_output=nout, pad=pad, weight_filler=weight_filler,\n",
    "                         bias_filler=bias_filler)\n",
    "    return conv, L.ReLU(conv, in_place=True)\n",
    "\n",
    "def vgg16_relu(bottom, nout, param=learned_param,\n",
    "            weight_filler=dict(type='xavier'),\n",
    "            bias_filler=dict(type='constant', value=0.1)):\n",
    "    fc = L.InnerProduct(bottom, num_output=nout,\n",
    "                        weight_filler=weight_filler,\n",
    "                        bias_filler=bias_filler)\n",
    "    return fc, L.ReLU(fc, in_place=True)\n",
    "\n",
    "def vgg16(data=None, label=None, train=True, num_classes=1000,\n",
    "             classifier_name='fc8', learn_all=False):\n",
    "    \"\"\"Returns a NetSpec specifying CaffeNet, following the original proto text\n",
    "       specification (./models/bvlc_reference_caffenet/train_val.prototxt).\"\"\"\n",
    "    n = caffe.NetSpec()\n",
    "    n.data = data\n",
    "    param = learned_param if learn_all else frozen_param\n",
    "    n.conv1_1, n.relu1_1 = vgg16_conv_relu(n.data, 3, 64, pad=1, param=param)\n",
    "    n.conv1_2, n.relu1_2 = vgg16_conv_relu(n.relu1_1, 3, 64, pad=1, param=param)\n",
    "    n.pool1 = max_pool(n.relu1_2, 2, stride=2)\n",
    "    n.conv2_1, n.relu2_1 = vgg16_conv_relu(n.pool1, 3, 128, pad=1, param=param)\n",
    "    n.conv2_2, n.relu2_2 = vgg16_conv_relu(n.relu2_1, 3, 128, pad=1, param=param)\n",
    "    n.pool2 = max_pool(n.relu2_2, 2, stride=2)\n",
    "    n.conv3_1, n.relu3_1 = vgg16_conv_relu(n.pool2, 3, 256, pad=1, param=param)\n",
    "    n.conv3_2, n.relu3_2 = vgg16_conv_relu(n.relu3_1, 3, 256, pad=1, param=param)\n",
    "    n.conv3_3, n.relu3_3 = vgg16_conv_relu(n.relu3_2, 3, 256, pad=1, param=param)\n",
    "    n.pool3 = max_pool(n.relu3_3, 2, stride=2)\n",
    "    n.conv4_1, n.relu4_1 = vgg16_conv_relu(n.pool3, 3, 512, pad=1, param=param)\n",
    "    n.conv4_2, n.relu4_2 = vgg16_conv_relu(n.relu4_1, 3, 512, pad=1, param=param)\n",
    "    n.conv4_3, n.relu4_3 = vgg16_conv_relu(n.relu4_2, 3, 512, pad=1, param=param)\n",
    "    n.pool4 = max_pool(n.relu4_3, 2, stride=2)\n",
    "    n.conv5_1, n.relu5_1 = vgg16_conv_relu(n.pool4, 3, 512, pad=1, param=param)\n",
    "    n.conv5_2, n.relu5_2 = vgg16_conv_relu(n.relu5_1, 3, 512, pad=1, param=param)\n",
    "    n.conv5_3, n.relu5_3 = vgg16_conv_relu(n.relu5_2, 3, 512, pad=1, param=param)\n",
    "    n.pool5 = max_pool(n.relu5_3, 2, stride=2)\n",
    "    n.fc6, n.relu6 = vgg16_relu(n.pool5, 4096, param=param)\n",
    "    if train:\n",
    "        n.drop6 = fc7input = L.Dropout(n.relu6, dropout_param=dict(dropout_ratio=0.5), in_place=True)\n",
    "    else:\n",
    "        fc7input = n.relu6\n",
    "\n",
    "    n.fc7, n.relu7 = vgg16_relu(fc7input, 4096, param=param)\n",
    "    if train:\n",
    "        n.drop7 = fc8input = L.Dropout(n.relu7, dropout_param=dict(dropout_ratio=0.5), in_place=True)\n",
    "    else:\n",
    "        fc8input = n.relu7\n",
    "\n",
    "    # always learn fc8 (param=learned_param)\n",
    "    fc8 = L.InnerProduct(fc8input, num_output=num_classes, weight_filler=dict(type='xavier'), bias_filler=dict(type='constant', value=0.1), param=learned_param)\n",
    "    # give fc8 the name specified by argument `classifier_name`\n",
    "    n.__setattr__(classifier_name, \"fc8\")\n",
    "    if not train:\n",
    "        n.probs = L.Softmax(fc8)\n",
    "    if label is not None:\n",
    "        n.label = label\n",
    "        n.loss = L.SoftmaxWithLoss(fc8, n.label)\n",
    "        n.acc = L.Accuracy(fc8, n.label)\n",
    "    # write the net to a temporary file and return its filename\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        f.write(str(n.to_proto()))\n",
    "        return f.name\n",
    "    \n",
    "def generic_prototext(data=None, label=None, train=True, num_classes=1000,\n",
    "             classifier_name='fc8', learn_all=False):\n",
    "    n = caffe.NetSpec()\n",
    "    n.data = data\n",
    "    n.label = label\n",
    "    \n",
    "    # give fc8 the name specified by argument `classifier_name`\n",
    "    #n.__setattr__(classifier_name, \"fc8\")\n",
    "    #if not train:\n",
    "    #    n.probs = L.Softmax(\"fc8\")\n",
    "    #if label is not None:\n",
    "    #    n.label = label\n",
    "    #    n.loss = L.SoftmaxWithLoss(\"fc8\", n.label)\n",
    "    #    n.acc = L.Accuracy(\"fc8\", n.label)\n",
    "        \n",
    "    # write the net to a temporary file and return its filename\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        f.write(str(n.to_proto()))\n",
    "        with open(input_proto, 'r') as proto:\n",
    "            f.write(str(proto.read()))\n",
    "        \n",
    "        if not train:\n",
    "            f.write(\"\"\"\n",
    "                layer {\n",
    "                  name: \"prob\"\n",
    "                  type: \"Softmax\"\n",
    "                  bottom: \"fc8_kaggle\"\n",
    "                  top: \"prob\"\n",
    "                }\n",
    "            \"\"\")\n",
    "        \n",
    "        f.write(\"\"\"\n",
    "            layer {\n",
    "              name: \"loss\"\n",
    "              type: \"SoftmaxWithLoss\"\n",
    "              bottom: \"fc8_kaggle\"\n",
    "              bottom: \"label\"\n",
    "              top: \"loss\"\n",
    "            }\n",
    "            layer {\n",
    "              name: \"acc\"\n",
    "              type: \"Accuracy\"\n",
    "              bottom: \"fc8_kaggle\"\n",
    "              bottom: \"label\"\n",
    "              top: \"acc\"\n",
    "            }\n",
    "        \"\"\")\n",
    "        \n",
    "        \n",
    "        return f.name\n",
    "    return input_proto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### define solver\n",
    "function solver to create our Caffe solvers, which are used to train the network (learn its weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "\n",
    "def solver(train_net_path, test_net_path=None, base_lr=0.001, stepsize=150, gamma=0.1):\n",
    "    s = caffe_pb2.SolverParameter()\n",
    "\n",
    "    # Specify locations of the train and (maybe) test networks.\n",
    "    s.train_net = train_net_path\n",
    "    if test_net_path is not None:\n",
    "        s.test_net.append(test_net_path)\n",
    "        s.test_interval = 1000  # Test after every 1000 training iterations.\n",
    "        s.test_iter.append(50) # Test on 100 batches each time we test.\n",
    "\n",
    "    # The number of iterations over which to average the gradient.\n",
    "    # Effectively boosts the training batch size by the given factor, without\n",
    "    # affecting memory utilization.\n",
    "    s.iter_size = 1\n",
    "    \n",
    "    s.max_iter = 100000     # # of times to update the net (training iterations)\n",
    "    \n",
    "    # Solve using the stochastic gradient descent (SGD) algorithm.\n",
    "    # Other choices include 'Adam' and 'RMSProp'.\n",
    "    s.type = 'SGD'\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    s.base_lr = base_lr\n",
    "\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # Here, we 'step' the learning rate by multiplying it by a factor `gamma`\n",
    "    # every `stepsize` iterations.\n",
    "    s.lr_policy = 'step'\n",
    "    s.gamma = gamma\n",
    "    s.stepsize = stepsize\n",
    "\n",
    "    # Set other SGD hyperparameters. Setting a non-zero `momentum` takes a\n",
    "    # weighted average of the current gradient and previous gradients to make\n",
    "    # learning more stable. L2 weight decay regularizes learning, to help prevent\n",
    "    # the model from overfitting.\n",
    "    s.momentum = 0.9\n",
    "    s.weight_decay = 5e-4\n",
    "\n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.  Here, we'll\n",
    "    # snapshot every 10K iterations -- ten times during training.\n",
    "    s.snapshot = 10000\n",
    "    s.snapshot_prefix = data_root + 'models'\n",
    "    \n",
    "    # Train on the GPU.  Using the CPU to train large networks is very slow.\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    # Write the solver to a temporary file and return its filename.\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        f.write(str(s))\n",
    "        return f.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define kaggle_net\n",
    "function kaggle_net which calls caffenet on data from the StateFarm dataset.\n",
    "\n",
    "The new network will also have the CaffeNet architecture, with differences in the input and output:\n",
    "\n",
    "* the input is the Kaggle StateFarm data we downloaded, provided by an ImageData layer\n",
    "* the output is a distribution over 10 classes rather than the original 1000 ImageNet classes\n",
    "* the classification layer is renamed from fc8 to fc8_kaggle to tell Caffe not to load the original classifier (fc8) weights from the ImageNet-pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kaggle_net(train=True, learn_all=True, batch_size=128):\n",
    "    source = data_root + 'train.txt' if train else 'valid.txt'\n",
    "    \n",
    "    transform_param = dict(mirror=train, crop_size=224,\n",
    "        mean_file=caffe_root + 'data/ilsvrc12/imagenet_mean.binaryproto')\n",
    "\n",
    "    kaggle_data, kaggle_label = L.ImageData(\n",
    "        transform_param=transform_param, source=source,\n",
    "        batch_size=batch_size, new_height=256, new_width=256, ntop=2)\n",
    "    \n",
    "    if input_proto != '':\n",
    "        return generic_prototext(data=kaggle_data, label=kaggle_label, train=train,\n",
    "                        num_classes=NUM_STATEFARM_LABELS,\n",
    "                        classifier_name='fc8_kaggle',\n",
    "                        learn_all=learn_all)\n",
    "    else:\n",
    "        return vgg16(data=kaggle_data, label=kaggle_label, train=train,\n",
    "                        num_classes=NUM_STATEFARM_LABELS,\n",
    "                        classifier_name='fc8_kaggle',\n",
    "                        learn_all=learn_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define run_solver\n",
    "run_solvers, a function that takes a list of solvers and steps each one in a round robin manner, recording the accuracy and loss values each iteration. At the end, the learned weights are saved to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_solvers(niter, solvers, disp_interval=100):\n",
    "    \"\"\"\n",
    "    Run solvers for niter iterations,\n",
    "    returning the loss and accuracy recorded each iteration.\n",
    "    'solvers' is a list of (name, solver) tuples.\n",
    "    \"\"\"\n",
    "    blobs = ('loss', 'acc')\n",
    "    loss, acc = ({name: np.zeros(niter) for name, _ in solvers}\n",
    "                 for _ in blobs)\n",
    "    for it in range(niter):\n",
    "        for name, s in solvers:\n",
    "            s.step(1)  # run a single SGD step in Caffe\n",
    "            loss[name][it], acc[name][it] = (s.net.blobs[b].data.copy()\n",
    "                                             for b in blobs)\n",
    "        if it % disp_interval == 0 or it + 1 == niter:\n",
    "            loss_disp = '; '.join('%s: loss=%.3f, acc=%2d%%' %\n",
    "                                  (n, loss[n][it], np.round(100*acc[n][it]))\n",
    "                                  for n, _ in solvers)\n",
    "            print '%3d) %s' % (it, loss_disp)     \n",
    "    # Save the learned weights from both nets.\n",
    "    weight_dir = tempfile.mkdtemp()\n",
    "    weights = {}\n",
    "    for name, s in solvers:\n",
    "        filename = 'weights.%s.caffemodel' % name\n",
    "        weights[name] = os.path.join(weight_dir, filename)\n",
    "        s.net.save(weights[name])\n",
    "    return loss, acc, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kaggle solver: train net initialized to the ImageNet-pretrained weights (this is done by the call to the copy_from method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/n7/ht1qjwk10v9bvp7mzwdq432m0000gn/T/tmpsMMiq_\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "niter = 600  # number of iterations to train\n",
    "# base_lr: starting learning rate = 0.001\n",
    "# learn_all: update the weights in all layers = True\n",
    "# stepsize: learning rate stepsize = 100\n",
    "\n",
    "# Reset kaggle_solver as before.\n",
    "kaggle_solver_filename = solver(kaggle_net())\n",
    "print(kaggle_solver_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running solvers for 600 iterations...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d70a8b06e3f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Running solvers for %d iterations...'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mniter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msolvers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pretrained'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkaggle_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_solvers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolvers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'Done.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a179ebcb07e7>\u001b[0m in \u001b[0;36mrun_solvers\u001b[0;34m(niter, solvers, disp_interval)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msolvers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run a single SGD step in Caffe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             loss[name][it], acc[name][it] = (s.net.blobs[b].data.copy()\n\u001b[1;32m     14\u001b[0m                                              for b in blobs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kaggle_solver = caffe.SGDSolver(kaggle_solver_filename)\n",
    "kaggle_solver.net.copy_from(weights)\n",
    "\n",
    "print 'Running solvers for %d iterations...' % niter\n",
    "solvers = [('pretrained', kaggle_solver)]\n",
    "loss, acc, weights = run_solvers(niter, solvers)\n",
    "print 'Done.'\n",
    "\n",
    "train_loss = loss['pretrained']\n",
    "train_acc = acc['pretrained']\n",
    "kaggle_weights = weights['pretrained']\n",
    "\n",
    "# Delete solvers to save memory.\n",
    "del kaggle_solver, solvers\n",
    "\n",
    "print \"Finished training the model in %f s\" % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-695cf1898b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration #'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot' is not defined"
     ]
    }
   ],
   "source": [
    "plot(np.vstack([train_loss]).T)\n",
    "xlabel('Iteration #')\n",
    "ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e98d95d4f2c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration #'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot' is not defined"
     ]
    }
   ],
   "source": [
    "plot(np.vstack([train_acc]).T)\n",
    "xlabel('Iteration #')\n",
    "ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_kaggle_net(weights, test_iters=40):\n",
    "    test_net = caffe.Net(kaggle_net(train=False), weights, caffe.TEST)\n",
    "    accuracy = 0\n",
    "    for it in xrange(test_iters):\n",
    "        accuracy += test_net.forward()['acc']\n",
    "    accuracy /= test_iters\n",
    "    return test_net, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy, trained from ImageNet initialization: 97.8%\n"
     ]
    }
   ],
   "source": [
    "test_net, accuracy = eval_kaggle_net(kaggle_weights)\n",
    "print 'Validation accuracy, trained from ImageNet initialization: %3.1f%%' % (100*accuracy, )\n",
    "del test_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_transformer(net):\n",
    "    mu = np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy')\n",
    "    mu = mu.mean(1).mean(1)  # average over pixels to obtain the mean (BGR) pixel values\n",
    "\n",
    "    # create transformer for the input called 'data'\n",
    "    transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n",
    "\n",
    "    transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension\n",
    "    transformer.set_mean('data', mu)            # subtract the dataset-mean value in each channel\n",
    "    transformer.set_raw_scale('data', 255)      # rescale from [0, 1] to [0, 255]\n",
    "    transformer.set_channel_swap('data', (2,1,0))  # swap channels from RGB to BGR\n",
    "    \n",
    "    return transformer\n",
    "\n",
    "def preprocess_batch(images, transformer):\n",
    "    transformed_images = map(\n",
    "        lambda image: transformer.preprocess('data', caffe.io.load_image(image)),\n",
    "        images\n",
    "    )\n",
    "    return transformed_images\n",
    "\n",
    "def process_batch(net, images):\n",
    "    net.blobs['data'].data[...] = images\n",
    "    probs = net.forward(start='conv1')['probs']\n",
    "    return probs\n",
    "\n",
    "def process_image(net, image):\n",
    "    net.blobs['data'].data[0, ...] = image\n",
    "    p = net.forward(start='conv1')['probs'][0]\n",
    "    return p\n",
    "\n",
    "def write_to_file(f_name, probs, images_name):\n",
    "    with open(data_root + f_name, \"a\") as f:\n",
    "        for i in xrange(len(probs)):\n",
    "            f.write(images_name[i][23:])\n",
    "            for p in probs[i]:\n",
    "                f.write(\",%f\" % p)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract probabilities for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_probabilities(net, image_set, batch_size=128, out_file='submission.txt', display_interval=10):\n",
    "    start_time = time.time()\n",
    "    with open(data_root + out_file, \"a\") as f:\n",
    "        f.write(\"img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9\\n\")\n",
    "    \n",
    "    transformer = initialize_transformer(net)\n",
    "    in_batch_size = (len(image_set)/batch_size)*batch_size    \n",
    "    \n",
    "    for s in range(0, in_batch_size, batch_size):\n",
    "        images = image_set[s:s+batch_size]\n",
    "        preprocessed_images = preprocess_batch(images, transformer)\n",
    "        probs = process_batch(net, preprocessed_images)\n",
    "        probs[probs < 0.00001] = 0\n",
    "        write_to_file(out_file, probs, images)\n",
    "        if (s / batch_size) % display_interval == 0:\n",
    "            print \"Processed %i images in %f sec\" % (s, time.time() - start_time)\n",
    "        \n",
    "    # for some reason reshaping the image data layer causes the kernel to crash    \n",
    "    left = image_set[in_batch_size:]\n",
    "    probs = []\n",
    "    for image in left:\n",
    "        preprocessed_image = transformer.preprocess('data', caffe.io.load_image(image))\n",
    "        probs_tmp = process_image(net, preprocessed_image)\n",
    "        probs_tmp[probs_tmp < 0.00001] = 0\n",
    "        probs.append(probs_tmp)\n",
    "    write_to_file(out_file, probs, left)\n",
    "    \n",
    "    print \"Finished extracting probabilities in %f sec\" % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_images = map(lambda name: data_root+'test/'+name, os.listdir(data_root+'test'))\n",
    "test_net = caffe.Net(kaggle_net(train=False), kaggle_weights, caffe.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images in 2.900404 sec\n",
      "Processed 1280 images in 31.159336 sec\n",
      "Processed 2560 images in 60.912330 sec\n",
      "Processed 3840 images in 90.451976 sec\n",
      "Processed 5120 images in 119.914245 sec\n",
      "Processed 6400 images in 149.083819 sec\n",
      "Processed 7680 images in 178.196667 sec\n",
      "Processed 8960 images in 207.226098 sec\n",
      "Processed 10240 images in 236.546783 sec\n",
      "Processed 11520 images in 265.918538 sec\n",
      "Processed 12800 images in 295.102234 sec\n",
      "Processed 14080 images in 324.207299 sec\n",
      "Processed 15360 images in 353.337148 sec\n",
      "Processed 16640 images in 383.233363 sec\n",
      "Processed 17920 images in 412.236261 sec\n",
      "Processed 19200 images in 441.205734 sec\n",
      "Processed 20480 images in 470.138082 sec\n",
      "Processed 21760 images in 499.137592 sec\n",
      "Processed 23040 images in 528.699056 sec\n",
      "Processed 24320 images in 558.050678 sec\n",
      "Processed 25600 images in 587.362133 sec\n",
      "Processed 26880 images in 616.728791 sec\n",
      "Processed 28160 images in 645.600376 sec\n",
      "Processed 29440 images in 674.482255 sec\n",
      "Processed 30720 images in 703.342180 sec\n",
      "Processed 32000 images in 732.279025 sec\n",
      "Processed 33280 images in 761.107370 sec\n",
      "Processed 34560 images in 788.702157 sec\n",
      "Processed 35840 images in 816.250975 sec\n",
      "Processed 37120 images in 844.084159 sec\n",
      "Processed 38400 images in 871.565117 sec\n",
      "Processed 39680 images in 899.130421 sec\n",
      "Processed 40960 images in 926.781765 sec\n",
      "Processed 42240 images in 954.358885 sec\n",
      "Processed 43520 images in 982.635863 sec\n",
      "Processed 44800 images in 1010.652148 sec\n",
      "Processed 46080 images in 1038.658837 sec\n",
      "Processed 47360 images in 1066.687771 sec\n",
      "Processed 48640 images in 1095.343513 sec\n",
      "Processed 49920 images in 1124.021068 sec\n",
      "Processed 51200 images in 1152.955700 sec\n",
      "Processed 52480 images in 1181.737942 sec\n",
      "Processed 53760 images in 1210.719228 sec\n",
      "Processed 55040 images in 1239.608141 sec\n",
      "Processed 56320 images in 1268.795534 sec\n",
      "Processed 57600 images in 1298.147338 sec\n",
      "Processed 58880 images in 1327.307969 sec\n",
      "Processed 60160 images in 1356.544984 sec\n",
      "Processed 61440 images in 1385.531120 sec\n",
      "Processed 62720 images in 1414.602681 sec\n",
      "Processed 64000 images in 1443.891373 sec\n",
      "Processed 65280 images in 1473.035281 sec\n",
      "Processed 66560 images in 1502.280804 sec\n",
      "Processed 67840 images in 1531.401904 sec\n",
      "Processed 69120 images in 1560.494487 sec\n",
      "Processed 70400 images in 1589.798708 sec\n",
      "Processed 71680 images in 1618.977598 sec\n",
      "Processed 72960 images in 1648.418560 sec\n",
      "Processed 74240 images in 1678.343628 sec\n",
      "Processed 75520 images in 1707.477022 sec\n",
      "Processed 76800 images in 1736.788148 sec\n",
      "Processed 78080 images in 1766.846661 sec\n",
      "Processed 79360 images in 1797.305519 sec\n",
      "Finished extracting probabilities in 1840.592933 sec\n"
     ]
    }
   ],
   "source": [
    "extract_probabilities(test_net, test_images)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
